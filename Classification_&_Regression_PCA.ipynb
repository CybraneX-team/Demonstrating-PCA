{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-LpkphF49aA"
      },
      "source": [
        "Using Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7ywRV4QktAt",
        "outputId": "993c02de-6cec-4815-cc30-f426148d60bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n",
            "Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "mbmwP1W_FhMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from category_encoders import LeaveOneOutEncoder, CatBoostEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Function to load data from CSV or JSON\n",
        "def load_data(file_path):\n",
        "    if file_path.endswith('.csv'):\n",
        "        return pd.read_csv(file_path)\n",
        "    elif file_path.endswith('.json'):\n",
        "        return pd.read_json(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please use CSV or JSON.\")\n",
        "\n",
        "# Function to perform data preprocessing for classification and regression data\n",
        "def preprocess_data(df, target_variable):\n",
        "    df_features = df.drop(columns=[target_variable])\n",
        "    df_features = df_features.fillna(df_features.median())  # Handle missing values\n",
        "    numerical_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = df_features.select_dtypes(include=[object, 'category']).columns.tolist()\n",
        "\n",
        "    encoder = None\n",
        "    if len(numerical_cols) > 0 and len(categorical_cols) == 0:\n",
        "        encoder = None\n",
        "    elif len(numerical_cols) == 0 and len(categorical_cols) > 0:\n",
        "        if all(df_features[col].nunique() < 10 for col in categorical_cols):\n",
        "            encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "        elif all(10 <= df_features[col].nunique() <= 100 for col in categorical_cols):\n",
        "            encoder = LeaveOneOutEncoder()\n",
        "        else:\n",
        "            encoder = CatBoostEncoder()\n",
        "    else:\n",
        "        if all(df_features[col].nunique() < 10 for col in categorical_cols):\n",
        "            encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "        elif all(10 <= df_features[col].nunique() <= 100 for col in categorical_cols):\n",
        "            encoder = LeaveOneOutEncoder()\n",
        "        else:\n",
        "            encoder = CatBoostEncoder()\n",
        "\n",
        "    if encoder is not None:\n",
        "        if isinstance(encoder, OneHotEncoder):\n",
        "            df_features_encoded = encoder.fit_transform(df_features[categorical_cols])\n",
        "            df_features = df_features.drop(columns=categorical_cols)\n",
        "            df_features = pd.concat([df_features, pd.DataFrame(df_features_encoded, columns=encoder.get_feature_names_out())], axis=1)\n",
        "        else:\n",
        "            df_features[categorical_cols] = encoder.fit_transform(df_features[categorical_cols], df[target_variable])\n",
        "\n",
        "    return df_features\n",
        "\n",
        "# Function to perform PCA and return explained variance ratio\n",
        "def perform_pca(df):\n",
        "    pca = PCA()\n",
        "    pca.fit(df)\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    return explained_variance_ratio, pca\n",
        "\n",
        "# Function to plot Scree plot\n",
        "def plot_scree(explained_variance_ratio):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio.cumsum(), marker='o', linestyle='--')\n",
        "    plt.title('Scree Plot')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "unfeHL5QFOvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Development and Evaluation"
      ],
      "metadata": {
        "id": "bSGzLTGZFkWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate models for classification\n",
        "def evaluate_classification_models(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),\n",
        "        'Support Vector Classifier': SVC(),\n",
        "        'XGBoost Classifier': XGBClassifier(n_estimators=100)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        results[name] = {\n",
        "            'Training time': training_time,\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1 Score': f1\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to evaluate models for regression\n",
        "def evaluate_regression_models(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Decision Tree': DecisionTreeRegressor(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100),\n",
        "        'Support Vector Regressor': SVR(),\n",
        "        'XGBoost Regressor': XGBRegressor(n_estimators=100)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "        y_pred = model.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        results[name] = {\n",
        "            'Training time': training_time,\n",
        "            'Mean Squared Error': mse,\n",
        "            'R^2 Score': r2,\n",
        "            'Mean Absolute Error': mae\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'your_dataset.csv'  # Replace with your file path\n",
        "df = load_data(file_path)\n",
        "target_variable = 'your_target_variable'\n",
        "dataset_type = 'classification'  # Change to 'regression' if the task is of regression\n",
        "\n",
        "# Preprocess data\n",
        "df_features = preprocess_data(df, target_variable)\n",
        "X = df_features.values\n",
        "y = df[target_variable].values\n",
        "\n",
        "# PCA analysis\n",
        "explained_variance_ratio, pca = perform_pca(df_features)\n",
        "plot_scree(explained_variance_ratio)\n",
        "total_variance = explained_variance_ratio.cumsum()[-1]\n",
        "print(f\"\\nTotal Variance Explained by PCA: {total_variance:.4f}\")\n",
        "\n",
        "# Split data and evaluate models\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "if dataset_type == 'regression':\n",
        "    evaluation_results = evaluate_regression_models(X_train, X_test, y_train, y_test)\n",
        "    print(\"\\nModel Evaluation Results for Regression:\")\n",
        "elif dataset_type == 'classification':\n",
        "    evaluation_results = evaluate_classification_models(X_train, X_test, y_train, y_test)\n",
        "    print(\"\\nModel Evaluation Results for Classification:\")\n",
        "\n",
        "# Print evaluation results\n",
        "for model_name, metrics in evaluation_results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "j7Ez_XSAFWFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YxuM3GuIp-He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X2ThmWzlp-ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import skew\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from category_encoders import LeaveOneOutEncoder, CatBoostEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Specifying the target variable\n",
        "target_variable = 'critical_temp'\n",
        "\n",
        "# Function to perform data preprocessing\n",
        "def preprocess_data(df, target_variable):\n",
        "    # Drop target variable for preprocessing\n",
        "    df_features = df.drop(columns=[target_variable])\n",
        "\n",
        "    # Handling missing values for numeric columns (replace with median)\n",
        "    df_features = df_features.fillna(df_features.median())\n",
        "\n",
        "    # Determine feature types\n",
        "    numerical_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = df_features.select_dtypes(include=[object, 'category']).columns.tolist()\n",
        "\n",
        "    # Initialize encoder\n",
        "    encoder = None\n",
        "\n",
        "    if len(numerical_cols) > 0 and len(categorical_cols) == 0:\n",
        "        # All numerical features\n",
        "        encoder = None\n",
        "    elif len(numerical_cols) == 0 and len(categorical_cols) > 0:\n",
        "        # All categorical features\n",
        "        if all(df_features[col].nunique() < 10 for col in categorical_cols):\n",
        "            encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "        elif all(10 <= df_features[col].nunique() <= 100 for col in categorical_cols):\n",
        "            encoder = LeaveOneOutEncoder()\n",
        "        else:\n",
        "            encoder = CatBoostEncoder()\n",
        "    else:\n",
        "        # Mixed features\n",
        "        if all(df_features[col].nunique() < 10 for col in categorical_cols):\n",
        "            encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "        elif all(10 <= df_features[col].nunique() <= 100 for col in categorical_cols):\n",
        "            encoder = LeaveOneOutEncoder()\n",
        "        else:\n",
        "            encoder = CatBoostEncoder()\n",
        "\n",
        "    if encoder is not None:\n",
        "        if isinstance(encoder, OneHotEncoder):\n",
        "            df_features_encoded = encoder.fit_transform(df_features[categorical_cols])\n",
        "            df_features = df_features.drop(columns=categorical_cols)\n",
        "            df_features = pd.concat([df_features, pd.DataFrame(df_features_encoded, columns=encoder.get_feature_names_out())], axis=1)\n",
        "        else:\n",
        "            df_features[categorical_cols] = encoder.fit_transform(df_features[categorical_cols], df[target_variable])\n",
        "\n",
        "    return df_features\n",
        "\n",
        "# Perform data preprocessing\n",
        "df_features = preprocess_data(df, target_variable)\n",
        "\n",
        "# StandardScaler for all columns except target variable\n",
        "scaler = StandardScaler()\n",
        "df_features_scaled = scaler.fit_transform(df_features)\n",
        "\n",
        "# Create a DataFrame from scaled data\n",
        "df_processed = pd.DataFrame(df_features_scaled, columns=df_features.columns)\n",
        "\n",
        "# Function to perform PCA and return explained variance ratio\n",
        "def perform_pca(df):\n",
        "    pca = PCA()\n",
        "    pca.fit(df)\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    return explained_variance_ratio, pca\n",
        "\n",
        "# Perform PCA on the processed data\n",
        "explained_variance, pca = perform_pca(df_processed)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Find the number of components required to reach 99% explained variance\n",
        "threshold = 0.99\n",
        "num_components = np.argmax(cumulative_explained_variance >= threshold) + 1\n",
        "\n",
        "total_explained_variance = sum(explained_variance)\n",
        "print(f\"\\nTotal Explained Variance: {total_explained_variance}\")\n",
        "# Determining the optimal number of principal components\n",
        "print(f\"Number of components to retain for capturing {threshold * 100}% variance: {num_components}\")\n",
        "\n",
        "# Transform the data using the selected number of components\n",
        "pca = PCA(n_components=num_components)\n",
        "transformed_data = pca.fit_transform(df_processed)\n",
        "\n",
        "print(\"\\nScree Plot of Principal Components capturing Total Variance: \\n\")\n",
        "\n",
        "# Generate the scree plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='b')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot')\n",
        "plt.axvline(x=num_components, color='r', linestyle='-')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Prepare data for model training\n",
        "X = transformed_data\n",
        "\n",
        "# Check if the target variable is continuous or categorical\n",
        "is_numeric = pd.api.types.is_numeric_dtype(df[target_variable])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "OPxr_2p_zlPf",
        "outputId": "aeba7fff-fca5-4a53-d38d-d9d9453291c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total Explained Variance: 1.0000000000000002\n",
            "Number of components to retain for capturing 99.0% variance: 31\n",
            "\n",
            "Scree Plot of Principal Components capturing Total Variance: \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Define skewness thresholds\n",
        "HIGH_SKEW_THRESHOLD = 1.0\n",
        "MODERATE_SKEW_THRESHOLD = 0.5\n",
        "\n",
        "# Function to calculate skewness and apply transformations based on skewness\n",
        "def calculate_skewness_and_transform(df, target_variable):\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    skewness_dict_before = {}\n",
        "    skewness_dict_after = {}\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col != target_variable:\n",
        "            original_skewness = skew(df[col].dropna())\n",
        "            skewness_dict_before[col] = original_skewness\n",
        "\n",
        "            if original_skewness > HIGH_SKEW_THRESHOLD:\n",
        "                # Apply log10 transformation if skewness is high\n",
        "                if (df[col] > 0).all():  # Ensure all values are positive\n",
        "                    df[col] = np.log10(df[col])\n",
        "                else:\n",
        "                    # Apply cube root transformation if log transformation is not applied\n",
        "                    df[col] = np.cbrt(df[col] + 1e-10)  # Add a small constant to avoid zero values\n",
        "\n",
        "            elif original_skewness > MODERATE_SKEW_THRESHOLD:\n",
        "                # Apply cube root transformation if skewness is moderate\n",
        "                df[col] = np.cbrt(df[col] + 1e-10)  # Add a small constant to avoid zero values\n",
        "\n",
        "            else:\n",
        "                # Apply square root transformation if skewness is low\n",
        "                df[col] = np.sqrt(df[col].clip(lower=0))\n",
        "\n",
        "            transformed_skewness = skew(df[col].dropna())\n",
        "            skewness_dict_after[col] = transformed_skewness\n",
        "\n",
        "    return skewness_dict_before, skewness_dict_after\n",
        "\n",
        "# Assume df is your existing dataframe and target_variable is specified\n",
        "# Calculate skewness and transform\n",
        "skewness_before, skewness_after = calculate_skewness_and_transform(df, target_variable)\n",
        "\n",
        "# Print skewness values before transformation\n",
        "print(\"\\nSkewness of the dataset before transformation: \\n\")\n",
        "for feature, skew_value in skewness_before.items():\n",
        "    print(f\"{feature}: {skew_value:.4f}\")\n",
        "\n",
        "# Print skewness values after transformation\n",
        "print(\"\\nSkewness of the dataset after transformation: \\n\")\n",
        "for feature, skew_value in skewness_after.items():\n",
        "    print(f\"{feature}: {skew_value:.4f}\")\n",
        "\n",
        "# Calculate and print skewness for the target variable separately before transformation\n",
        "target_skewness_before = skew(df[target_variable].dropna())\n",
        "print(f\"\\nSkewness of the target variable '{target_variable}' before transformation: {target_skewness_before:.4f}\")\n",
        "\n",
        "# Apply square root transformation to the target variable\n",
        "df[target_variable] = np.sqrt(df[target_variable].clip(lower=0))\n",
        "\n",
        "# Calculate and print skewness for the target variable after transformation\n",
        "target_skewness_after = skew(df[target_variable].dropna())\n",
        "print(f\"\\nSkewness of the target variable '{target_variable}' after transformation: {target_skewness_after:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQdnjfLDr4dU",
        "outputId": "7c29a77f-6436-4b6c-84a4-7fff27c95601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Skewness of the dataset before transformation: \n",
            "\n",
            "number_of_elements: 0.0092\n",
            "mean_atomic_mass: 0.7746\n",
            "wtd_mean_atomic_mass: 1.5038\n",
            "gmean_atomic_mass: 1.4747\n",
            "wtd_gmean_atomic_mass: 1.6982\n",
            "entropy_atomic_mass: -0.7852\n",
            "wtd_entropy_atomic_mass: -0.6268\n",
            "range_atomic_mass: -0.3771\n",
            "wtd_range_atomic_mass: 2.4636\n",
            "std_atomic_mass: -0.3483\n",
            "wtd_std_atomic_mass: -0.3083\n",
            "mean_fie: 1.0097\n",
            "wtd_mean_fie: -0.2897\n",
            "gmean_fie: 1.1918\n",
            "wtd_gmean_fie: -0.3148\n",
            "entropy_fie: -0.7925\n",
            "wtd_entropy_fie: -0.0303\n",
            "range_fie: -0.4563\n",
            "wtd_range_fie: -0.2780\n",
            "std_fie: -0.4525\n",
            "wtd_std_fie: -0.4099\n",
            "mean_atomic_radius: -0.5655\n",
            "wtd_mean_atomic_radius: 0.5668\n",
            "gmean_atomic_radius: 0.0921\n",
            "wtd_gmean_atomic_radius: 0.5526\n",
            "entropy_atomic_radius: -0.7498\n",
            "wtd_entropy_atomic_radius: -0.7891\n",
            "range_atomic_radius: -0.6875\n",
            "wtd_range_atomic_radius: 1.6430\n",
            "std_atomic_radius: -0.7492\n",
            "wtd_std_atomic_radius: -0.6459\n",
            "mean_Density: 2.1032\n",
            "wtd_mean_Density: 2.1376\n",
            "gmean_Density: 1.6590\n",
            "wtd_gmean_Density: 1.6508\n",
            "entropy_Density: -0.8074\n",
            "wtd_entropy_Density: -0.6025\n",
            "range_Density: 0.3751\n",
            "wtd_range_Density: 3.3225\n",
            "std_Density: 0.8606\n",
            "wtd_std_Density: 0.4547\n",
            "mean_ElectronAffinity: 1.0984\n",
            "wtd_mean_ElectronAffinity: -0.2160\n",
            "gmean_ElectronAffinity: 1.3496\n",
            "wtd_gmean_ElectronAffinity: 0.4132\n",
            "entropy_ElectronAffinity: -0.9123\n",
            "wtd_entropy_ElectronAffinity: -0.2305\n",
            "range_ElectronAffinity: 0.9433\n",
            "wtd_range_ElectronAffinity: 0.1146\n",
            "std_ElectronAffinity: 0.5774\n",
            "wtd_std_ElectronAffinity: 0.5222\n",
            "mean_FusionHeat: 2.5226\n",
            "wtd_mean_FusionHeat: 2.7636\n",
            "gmean_FusionHeat: 2.6653\n",
            "wtd_gmean_FusionHeat: 2.4874\n",
            "entropy_FusionHeat: -0.5787\n",
            "wtd_entropy_FusionHeat: -0.5719\n",
            "range_FusionHeat: 2.7484\n",
            "wtd_range_FusionHeat: 4.2333\n",
            "std_FusionHeat: 2.8224\n",
            "wtd_std_FusionHeat: 2.8499\n",
            "mean_ThermalConductivity: 0.2221\n",
            "wtd_mean_ThermalConductivity: 1.3796\n",
            "gmean_ThermalConductivity: 2.3398\n",
            "wtd_gmean_ThermalConductivity: 2.5881\n",
            "entropy_ThermalConductivity: -0.1215\n",
            "wtd_entropy_ThermalConductivity: 0.3113\n",
            "range_ThermalConductivity: -0.2343\n",
            "wtd_range_ThermalConductivity: 1.4158\n",
            "std_ThermalConductivity: -0.2278\n",
            "wtd_std_ThermalConductivity: -0.0880\n",
            "mean_Valence: 1.0036\n",
            "wtd_mean_Valence: 0.8946\n",
            "gmean_Valence: 1.1684\n",
            "wtd_gmean_Valence: 1.0167\n",
            "entropy_Valence: -0.7732\n",
            "wtd_entropy_Valence: -0.7293\n",
            "range_Valence: 0.4694\n",
            "wtd_range_Valence: 1.5475\n",
            "std_Valence: 0.4448\n",
            "wtd_std_Valence: 0.6579\n",
            "\n",
            "Skewness of the dataset after transformation: \n",
            "\n",
            "number_of_elements: -0.3625\n",
            "mean_atomic_mass: -0.5491\n",
            "wtd_mean_atomic_mass: -0.5170\n",
            "gmean_atomic_mass: -0.7430\n",
            "wtd_gmean_atomic_mass: 0.1983\n",
            "entropy_atomic_mass: -2.1117\n",
            "wtd_entropy_atomic_mass: -1.6480\n",
            "range_atomic_mass: -1.1865\n",
            "wtd_range_atomic_mass: -0.1126\n",
            "std_atomic_mass: -1.2574\n",
            "wtd_std_atomic_mass: -1.1458\n",
            "mean_fie: 0.3957\n",
            "wtd_mean_fie: -0.3836\n",
            "gmean_fie: 0.6394\n",
            "wtd_gmean_fie: -0.4257\n",
            "entropy_fie: -2.2953\n",
            "wtd_entropy_fie: -1.6590\n",
            "range_fie: -0.9531\n",
            "wtd_range_fie: -0.9934\n",
            "std_fie: -1.0481\n",
            "wtd_std_fie: -0.8482\n",
            "mean_atomic_radius: -0.9025\n",
            "wtd_mean_atomic_radius: 0.3344\n",
            "gmean_atomic_radius: -0.2869\n",
            "wtd_gmean_atomic_radius: 0.3599\n",
            "entropy_atomic_radius: -2.2598\n",
            "wtd_entropy_atomic_radius: -1.7945\n",
            "range_atomic_radius: -1.2313\n",
            "wtd_range_atomic_radius: -0.5061\n",
            "std_atomic_radius: -1.4194\n",
            "wtd_std_atomic_radius: -1.2534\n",
            "mean_Density: -0.2482\n",
            "wtd_mean_Density: 0.3077\n",
            "gmean_Density: -0.2887\n",
            "wtd_gmean_Density: -0.0757\n",
            "entropy_Density: -2.3040\n",
            "wtd_entropy_Density: -1.8619\n",
            "range_Density: -1.0056\n",
            "wtd_range_Density: 0.0593\n",
            "std_Density: -1.7405\n",
            "wtd_std_Density: -1.0157\n",
            "mean_ElectronAffinity: -2.6221\n",
            "wtd_mean_ElectronAffinity: -1.0642\n",
            "gmean_ElectronAffinity: -0.9999\n",
            "wtd_gmean_ElectronAffinity: -0.5159\n",
            "entropy_ElectronAffinity: -2.1206\n",
            "wtd_entropy_ElectronAffinity: -1.7128\n",
            "range_ElectronAffinity: -1.8259\n",
            "wtd_range_ElectronAffinity: -0.9439\n",
            "std_ElectronAffinity: -2.1120\n",
            "wtd_std_ElectronAffinity: -2.0008\n",
            "mean_FusionHeat: 0.7601\n",
            "wtd_mean_FusionHeat: 0.6341\n",
            "gmean_FusionHeat: 0.6327\n",
            "wtd_gmean_FusionHeat: 0.2893\n",
            "entropy_FusionHeat: -1.8184\n",
            "wtd_entropy_FusionHeat: -1.5454\n",
            "range_FusionHeat: 0.3692\n",
            "wtd_range_FusionHeat: 1.0063\n",
            "std_FusionHeat: 0.6426\n",
            "wtd_std_FusionHeat: 0.2474\n",
            "mean_ThermalConductivity: -0.7067\n",
            "wtd_mean_ThermalConductivity: -1.7805\n",
            "gmean_ThermalConductivity: -0.2886\n",
            "wtd_gmean_ThermalConductivity: 0.1172\n",
            "entropy_ThermalConductivity: -1.1311\n",
            "wtd_entropy_ThermalConductivity: -0.4218\n",
            "range_ThermalConductivity: -0.5608\n",
            "wtd_range_ThermalConductivity: -0.7020\n",
            "std_ThermalConductivity: -0.6171\n",
            "wtd_std_ThermalConductivity: -0.4727\n",
            "mean_Valence: 0.5441\n",
            "wtd_mean_Valence: 0.6399\n",
            "gmean_Valence: 0.6993\n",
            "wtd_gmean_Valence: 0.6338\n",
            "entropy_Valence: -2.1614\n",
            "wtd_entropy_Valence: -1.8070\n",
            "range_Valence: -0.7264\n",
            "wtd_range_Valence: -0.7161\n",
            "std_Valence: -0.8794\n",
            "wtd_std_Valence: -1.3437\n",
            "\n",
            "Skewness of the target variable 'critical_temp' before transformation: 0.8602\n",
            "\n",
            "Skewness of the target variable 'critical_temp' after transformation: 0.3174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate different models for classification\n",
        "def evaluate_classification_models(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),\n",
        "        'Support Vector Classifier': SVC(),\n",
        "        'XGBoost Classifier': XGBClassifier(n_estimators=100)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Evaluation metrics for classification task: Precision, Recall, F1-Score\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        results[name] = {\n",
        "            'Training time': training_time,\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall, 'F1 Score': f1\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to evaluate different models for regression\n",
        "def evaluate_regression_models(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Decision Tree': DecisionTreeRegressor(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100),\n",
        "        'Support Vector Regressor': SVR(),\n",
        "        'XGBoost Regressor': XGBRegressor(n_estimators=100)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Evaluation metrics for regression task: MSE, R^2, MAE\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "        y_pred = model.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        results[name] = {\n",
        "            'Training time': training_time,\n",
        "            'Mean Squared Error': mse,\n",
        "            'R^2 Score': r2,\n",
        "            'Mean Absolute Error': mae\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# If the target variable is numeric, perform regression\n",
        "if is_numeric:\n",
        "    y = df[target_variable].astype(float)\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    # Evaluate regression models\n",
        "    evaluation_results = evaluate_regression_models(X_train, X_test, y_train, y_test)\n",
        "    print(\"\\nModel Evaluation Results for Regression:\")\n",
        "else:\n",
        "    # Convert the target variable to numeric values starting from 0 for classification\n",
        "    y = df[target_variable].str.strip(\"'\").astype(int) - 1\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    # Evaluate classification models\n",
        "    evaluation_results = evaluate_classification_models(X_train, X_test, y_train, y_test)\n",
        "    print(\"\\nModel Evaluation Results for Classification:\")\n",
        "\n",
        "# Print the evaluation results\n",
        "for model_name, metrics in evaluation_results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtFohkKjaapr",
        "outputId": "ebf98c26-6f2c-4011-86a9-10ba6f0c9e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Evaluation Results for Regression:\n",
            "\n",
            "Linear Regression:\n",
            "  Training time: 0.1515\n",
            "  Mean Squared Error: 2.3329\n",
            "  R^2 Score: 0.7442\n",
            "  Mean Absolute Error: 1.2086\n",
            "\n",
            "Decision Tree:\n",
            "  Training time: 1.1790\n",
            "  Mean Squared Error: 1.0445\n",
            "  R^2 Score: 0.8855\n",
            "  Mean Absolute Error: 0.5930\n",
            "\n",
            "Random Forest:\n",
            "  Training time: 74.6792\n",
            "  Mean Squared Error: 0.6424\n",
            "  R^2 Score: 0.9296\n",
            "  Mean Absolute Error: 0.5042\n",
            "\n",
            "Gradient Boosting:\n",
            "  Training time: 26.0578\n",
            "  Mean Squared Error: 1.4585\n",
            "  R^2 Score: 0.8401\n",
            "  Mean Absolute Error: 0.9159\n",
            "\n",
            "Support Vector Regressor:\n",
            "  Training time: 27.4647\n",
            "  Mean Squared Error: 1.3483\n",
            "  R^2 Score: 0.8522\n",
            "  Mean Absolute Error: 0.7700\n",
            "\n",
            "XGBoost Regressor:\n",
            "  Training time: 7.9299\n",
            "  Mean Squared Error: 0.7336\n",
            "  R^2 Score: 0.9196\n",
            "  Mean Absolute Error: 0.5705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate different models\n",
        "def evaluate_models(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),\n",
        "        'Support Vector Classifier': SVC(),\n",
        "        'XGBoost Classifier': XGBClassifier(n_estimators=100)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Evaluation metrics for classification task: Precision, Recall, F1-Score\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        results[name] = {\n",
        "            'Training time': training_time,\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall, 'F1 Score': f1\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Evaluate models and print results\n",
        "evaluation_results = evaluate_models(X_train, X_test, y_train, y_test)\n",
        "\n",
        "print(\"\\nModel Evaluation Results after PCA:\")\n",
        "for model_name, metrics in evaluation_results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9j-98jrBwn2",
        "outputId": "5f4daaa4-b684-45f3-d89a-f51def237ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Evaluation Results after PCA:\n",
            "\n",
            "Logistic Regression:\n",
            "  Training time: 14.8069\n",
            "  Accuracy: 0.9500\n",
            "  Precision: 0.9505\n",
            "  Recall: 0.9500\n",
            "  F1 Score: 0.9500\n",
            "\n",
            "Decision Tree:\n",
            "  Training time: 1.2527\n",
            "  Accuracy: 0.6840\n",
            "  Precision: 0.6835\n",
            "  Recall: 0.6840\n",
            "  F1 Score: 0.6826\n",
            "\n",
            "Random Forest:\n",
            "  Training time: 11.4478\n",
            "  Accuracy: 0.8929\n",
            "  Precision: 0.8989\n",
            "  Recall: 0.8929\n",
            "  F1 Score: 0.8928\n",
            "\n",
            "Gradient Boosting:\n",
            "  Training time: 853.8476\n",
            "  Accuracy: 0.8744\n",
            "  Precision: 0.8801\n",
            "  Recall: 0.8744\n",
            "  F1 Score: 0.8747\n",
            "\n",
            "Support Vector Classifier:\n",
            "  Training time: 1.0980\n",
            "  Accuracy: 0.9583\n",
            "  Precision: 0.9606\n",
            "  Recall: 0.9583\n",
            "  F1 Score: 0.9586\n",
            "\n",
            "XGBoost Classifier:\n",
            "  Training time: 27.1918\n",
            "  Accuracy: 0.8994\n",
            "  Precision: 0.9010\n",
            "  Recall: 0.8994\n",
            "  F1 Score: 0.8994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformed_data)\n",
        "print(\"\\nShape of the Transformed Data:\", transformed_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEJ7_EYbhZX6",
        "outputId": "e8674fc9-08b0-4054-e4a0-0b12b839e0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -9.4195468    0.45994744  -7.28877649 ...   0.41077129   1.16460158\n",
            "   -0.58495495]\n",
            " [ -5.12472771  -1.29713081  -7.83794222 ...  -1.15019213  -0.16091379\n",
            "    0.18903053]\n",
            " [-13.4003315    1.89319021 -13.89587765 ...   0.11297229   0.19668616\n",
            "    1.25418407]\n",
            " ...\n",
            " [  8.74739933  -1.00326082  -4.59571153 ...   0.74572811  -0.85893578\n",
            "    0.4681231 ]\n",
            " [-16.39386212   3.48123473   8.12071514 ...  -0.40756986  -0.75843866\n",
            "    0.1770216 ]\n",
            " [-16.57300422  -1.36791895   8.08999793 ...   0.20412602   0.19241238\n",
            "   -1.23132401]]\n",
            "\n",
            "Shape of the Transformed Data: (7797, 114)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Regression Tasks"
      ],
      "metadata": {
        "id": "P2PdF4DSCOrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from category_encoders import LeaveOneOutEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset (replace with your actual dataset loading code)\n",
        "df = pd.read_csv('Insurance_Claims.csv')\n",
        "\n",
        "# Assuming 'CLM_PMT_AMT' is your target variable\n",
        "target_variable = 'CLM_PMT_AMT'\n",
        "\n",
        "# Function to perform data preprocessing\n",
        "def preprocess_data(df):\n",
        "    # Drop the first 3 columns\n",
        "    df = df.drop(columns=df.columns[:3])\n",
        "\n",
        "    # Print the type of each column in the dataset\n",
        "    print(\"\\nData Types of Each Column in the Health Insurance Claims Dataset: \\n\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Separate numeric and categorical columns\n",
        "    numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_columns = df.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "    # Convert date columns to datetime\n",
        "    date_columns = ['CLM_FROM_DT', 'CLM_THRU_DT', 'CLM_ADMSN_DT', 'NCH_BENE_DSCHRG_DT']\n",
        "    for col in date_columns:\n",
        "        df[col] = pd.to_datetime(df[col], format='%Y%m%d')\n",
        "\n",
        "    # Handling missing values for numeric columns (for demonstration, replace with median)\n",
        "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())\n",
        "\n",
        "    # Handling missing values for categorical columns (for demonstration, replace with mode using forward fill)\n",
        "    df[categorical_columns] = df[categorical_columns].fillna(method='ffill')\n",
        "\n",
        "    # Extract features from date columns\n",
        "    for col in date_columns:\n",
        "        df[f'{col}_year'] = df[col].dt.year\n",
        "        df[f'{col}_month'] = df[col].dt.month\n",
        "        df[f'{col}_day'] = df[col].dt.day\n",
        "\n",
        "    # Drop original date columns after extracting features\n",
        "    df = df.drop(columns=date_columns)\n",
        "\n",
        "    # Update numeric and categorical columns after feature extraction\n",
        "    numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    categorical_columns = df.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "    # Encoding categorical variables using LeaveOneOutEncoder\n",
        "    encoder = LeaveOneOutEncoder()\n",
        "    encoded_categorical = encoder.fit_transform(df[categorical_columns], df[target_variable])\n",
        "\n",
        "    # Combine numeric and encoded categorical data into a single DataFrame\n",
        "    df_processed = pd.concat([df[numeric_columns], encoded_categorical], axis=1)\n",
        "\n",
        "    # Extract updated column names after preprocessing\n",
        "    updated_columns = numeric_columns + list(encoded_categorical.columns)\n",
        "\n",
        "    return df_processed, updated_columns\n",
        "\n",
        "# Perform data preprocessing\n",
        "df_processed, updated_columns = preprocess_data(df)\n",
        "\n",
        "# StandardScaler for numeric columns\n",
        "scaler = StandardScaler()\n",
        "numeric_cols_to_scale = [col for col in updated_columns if col != target_variable]\n",
        "df_processed[numeric_cols_to_scale] = scaler.fit_transform(df_processed[numeric_cols_to_scale])\n",
        "\n",
        "# Function to calculate correlation for numeric columns\n",
        "def calculate_numeric_correlation(df, numeric_cols, target_col):\n",
        "    correlations = {}\n",
        "    for col in numeric_cols:\n",
        "        if col != target_col:\n",
        "            corr = df[col].corr(df[target_col])\n",
        "            correlations[col] = corr\n",
        "    return correlations\n",
        "\n",
        "# Calculate correlations (only numeric correlations)\n",
        "numeric_correlations = calculate_numeric_correlation(df_processed, numeric_cols_to_scale, target_variable)\n",
        "\n",
        "# Create a DataFrame for correlation results\n",
        "correlation_results = pd.DataFrame({'Feature': list(numeric_correlations.keys()),\n",
        "                                    'Correlation': list(numeric_correlations.values())})\n",
        "\n",
        "# Print the type of each column in the dataset\n",
        "print(\"\\nData Types of Each Column in the Dataset after Leave-One-Out Encoding: \\n\")\n",
        "print(df_processed.dtypes)\n",
        "\n",
        "# Print or visualize the correlation results\n",
        "print(\"\\nPairwise Correlations between the Features and the Target Variable: \\n\")\n",
        "print(correlation_results)\n",
        "\n",
        "# Function to calculate VIF\n",
        "def calculate_vif(df):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = df.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
        "    return vif_data\n",
        "\n",
        "# Calculate VIF for numeric columns\n",
        "vif_results = calculate_vif(df_processed[numeric_cols_to_scale])\n",
        "\n",
        "# Print VIF results\n",
        "print(\"\\nVariance Inflation Factor (VIF) Estimates: \\n\")\n",
        "print(vif_results)\n",
        "\n",
        "# Function to perform PCA and return explained variance ratio\n",
        "def perform_pca(df):\n",
        "    pca = PCA()\n",
        "    pca.fit(df)\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    return explained_variance_ratio, pca\n",
        "\n",
        "# Perform PCA on the processed data\n",
        "explained_variance, pca = perform_pca(df_processed[numeric_cols_to_scale])\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"\\nExplained Variance Ratio after PCA: \\n\")\n",
        "print(explained_variance)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Find the number of components required to reach 90% explained variance\n",
        "threshold = 0.90\n",
        "num_components = np.argmax(cumulative_explained_variance >= threshold) + 1\n",
        "\n",
        "total_explained_variance = sum(explained_variance)\n",
        "print(f\"\\nTotal Explained Variance: {total_explained_variance}\")\n",
        "# Determining the optimal number of principal components\n",
        "print(f\"Number of components to retain for capturing {threshold * 100}% variance: {num_components}\")\n",
        "\n",
        "# Transform the data using the selected number of components\n",
        "pca = PCA(n_components=num_components)\n",
        "transformed_data = pca.fit_transform(df_processed[numeric_cols_to_scale])\n",
        "\n",
        "print(\"\\nScree Plot of Principal Components capturing Total Variance: \\n\")\n",
        "\n",
        "# Generate the scree plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='b')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot')\n",
        "plt.axvline(x=num_components, color='r', linestyle='-')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vbNID0B0DWaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Split data into train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(transformed_data, df_processed[target_variable], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Decision Tree': DecisionTreeRegressor(),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100),\n",
        "    'Support Vector Regressor': SVR(),\n",
        "    'XGBoost Regressor': XGBRegressor(n_estimators=100)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\n",
        "        'Training time': end_time - start_time,\n",
        "        'Mean Squared Error': mse,\n",
        "        'R^2 Score': r2,\n",
        "        'Mean Absolute Error': mae\n",
        "    }\n",
        "\n",
        "# Print results\n",
        "print(\"\\nModel Evaluation Results after PCA:\")\n",
        "for name, result in results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Training time: {result['Training time']:.4f} seconds\")\n",
        "    print(f\"  Mean Squared Error: {result['Mean Squared Error']:.4f}\")\n",
        "    print(f\"  R^2 Score: {result['R^2 Score']:.4f}\")\n",
        "    print(f\"  Mean Absolute Error: {result['Mean Absolute Error']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNpJzCjrXwUY",
        "outputId": "b1e0ee27-7225-4555-9456-caba3c187fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results after PCA:\n",
            "Linear Regression:\n",
            "  Training time: 0.1088 seconds\n",
            "  Mean Squared Error: 51153529.8744\n",
            "  R^2 Score: 0.3960\n",
            "  Mean Absolute Error: 4485.8350\n",
            "Decision Tree:\n",
            "  Training time: 3.2528 seconds\n",
            "  Mean Squared Error: 107855621.5125\n",
            "  R^2 Score: -0.2735\n",
            "  Mean Absolute Error: 6365.6541\n",
            "Random Forest:\n",
            "  Training time: 237.3401 seconds\n",
            "  Mean Squared Error: 50602940.7020\n",
            "  R^2 Score: 0.4025\n",
            "  Mean Absolute Error: 4431.0474\n",
            "Gradient Boosting:\n",
            "  Training time: 79.4153 seconds\n",
            "  Mean Squared Error: 48933938.0306\n",
            "  R^2 Score: 0.4222\n",
            "  Mean Absolute Error: 4298.5895\n",
            "Support Vector Regressor:\n",
            "  Training time: 212.5169 seconds\n",
            "  Mean Squared Error: 85629222.8188\n",
            "  R^2 Score: -0.0110\n",
            "  Mean Absolute Error: 5183.7159\n",
            "XGBoost:\n",
            "  Training time: 1.9416 seconds\n",
            "  Mean Squared Error: 51186518.7699\n",
            "  R^2 Score: 0.3956\n",
            "  Mean Absolute Error: 4391.3102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEgSeYRn46aq"
      },
      "source": [
        "Without Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3RBmIRXGD88",
        "outputId": "e457ee0b-6327-484a-ada9-e698d4b85391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Evaluation Results:\n",
            "\n",
            "Logistic Regression:\n",
            "  Training time: 69.8556\n",
            "  Accuracy: 0.9571\n",
            "  Precision: 0.9578\n",
            "  Recall: 0.9571\n",
            "  F1 Score: 0.9572\n",
            "\n",
            "Decision Tree:\n",
            "  Training time: 6.3121\n",
            "  Accuracy: 0.8160\n",
            "  Precision: 0.8174\n",
            "  Recall: 0.8160\n",
            "  F1 Score: 0.8155\n",
            "\n",
            "Random Forest:\n",
            "  Training time: 15.9664\n",
            "  Accuracy: 0.9429\n",
            "  Precision: 0.9444\n",
            "  Recall: 0.9429\n",
            "  F1 Score: 0.9429\n",
            "\n",
            "Gradient Boosting:\n",
            "  Training time: 3522.7400\n",
            "  Accuracy: 0.9321\n",
            "  Precision: 0.9340\n",
            "  Recall: 0.9321\n",
            "  F1 Score: 0.9322\n",
            "\n",
            "Support Vector Classifier:\n",
            "  Training time: 6.0345\n",
            "  Accuracy: 0.9622\n",
            "  Precision: 0.9639\n",
            "  Recall: 0.9622\n",
            "  F1 Score: 0.9624\n",
            "\n",
            "XGBoost Classifier:\n",
            "  Training time: 125.2099\n",
            "  Accuracy: 0.9468\n",
            "  Precision: 0.9487\n",
            "  Recall: 0.9468\n",
            "  F1 Score: 0.9470\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Numerical_Data.csv')\n",
        "\n",
        "# Specifying the target variable\n",
        "target_variable = 'class'\n",
        "\n",
        "# Function to perform data preprocessing\n",
        "def preprocess_data(df, target_variable):\n",
        "    # Drop target variable for preprocessing\n",
        "    df_features = df.drop(columns=[target_variable])\n",
        "\n",
        "    # Handling missing values for numeric columns (replace with median)\n",
        "    df_features = df_features.fillna(df_features.median())\n",
        "\n",
        "    return df_features\n",
        "\n",
        "# Perform data preprocessing\n",
        "df_features = preprocess_data(df, target_variable)\n",
        "\n",
        "# StandardScaler for all columns except target variable\n",
        "scaler = StandardScaler()\n",
        "df_features_scaled = scaler.fit_transform(df_features)\n",
        "\n",
        "# Create a DataFrame from scaled data\n",
        "df_processed = pd.DataFrame(df_features_scaled, columns=df_features.columns)\n",
        "\n",
        "# Prepare data for model training\n",
        "X = df_processed.values\n",
        "\n",
        "# Convert the target variable to numeric values starting from 0\n",
        "y = df[target_variable].str.strip(\"'\").astype(int) - 1\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to evaluate different models\n",
        "def evaluate_models(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=2000, solver='lbfgs'),\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100),\n",
        "        'Support Vector Classifier': SVC(),\n",
        "        'XGBoost Classifier': XGBClassifier(n_estimators=100)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        results[name] = {\n",
        "            'Training time': training_time,\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall, 'F1 Score': f1\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Evaluate models and print results\n",
        "evaluation_results = evaluate_models(X_train, X_test, y_train, y_test)\n",
        "\n",
        "print(\"\\nModel Evaluation Results:\")\n",
        "for model_name, metrics in evaluation_results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}